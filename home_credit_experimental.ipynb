{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "df_train = pd.read_csv('home_credit_train_engineered.csv')\n",
    "\n",
    "df_train.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining features\n",
    "features = [f for f in df_train.columns if f not in ['Unnamed: 0','TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting train test splits and scaling data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "df_train[features], df_train['TARGET'], test_size=0.33, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original train set shape: 6951\nResample train set shape : 1460\n"
     ]
    }
   ],
   "source": [
    "# Getting undersampled data\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\n",
    "x_u, y_u = rus.fit_resample(x_train, y_train)\n",
    "\n",
    "print('Original train set shape:', len(x_train))\n",
    "print('Resample train set shape :', len(x_r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original train set shape: 6951\nResample train set shape : 12442\n"
     ]
    }
   ],
   "source": [
    "# Getting oversampled data\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "x_o, y_o = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "print('Original train set shape:', len(x_train))\n",
    "print('Resample train set shape :', len(x_o))\n",
    "\n",
    "\n",
    "\n",
    "# print('\\nBalance of positive and negative classes (%):')\n",
    "# y_sm.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_classifier(clf,x_train = x_train,y_train = y_train,x_test = x_test,y_test = y_test):\n",
    "    \n",
    "    print(\"Classification on data from dataset\\n\")\n",
    "    clf.fit(x_train,y_train)\n",
    "    print(\"\\n\\nTraining report\")\n",
    "    train_report = classification_report(y_train,clf.predict(x_train))\n",
    "    print(train_report)\n",
    "    print(\"\\n\\nTesting report\")\n",
    "    test_report = classification_report(y_test,clf.predict(x_test))\n",
    "    print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampled_classifier(clf,x_u = x_u,y_u = y_u,x_test = x_test,y_test = y_test):\n",
    "    print(\"Classification on undersampled data\\n\")\n",
    "    clf.fit(x_u,y_u)\n",
    "    print(\"\\n\\nTraining report\")\n",
    "    train_report = classification_report(y_u,clf.predict(x_u))\n",
    "    print(train_report)\n",
    "    print(\"\\n\\nTesting report\")\n",
    "    test_report = classification_report(y_test,clf.predict(x_test))\n",
    "    print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampled_classifier(clf,x_o=x_o,y_=y_o,x_test=x_test,y_test=y_test):\n",
    "    print(\"\\n\\n For Oversampled data\\n\")\n",
    "    clf.fit(x_o,y_o)\n",
    "    print(\"\\n\\nTraining report\")\n",
    "    train_report = classification_report(y_o,clf.predict(x_o))\n",
    "    print(train_report)\n",
    "    print(\"\\n\\nTesting report\")\n",
    "    test_report = classification_report(y_test,clf.predict(x_test))\n",
    "    print(test_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM classifier\n",
    "import lightgbm as ltb\n",
    "model = ltb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on data from dataset\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      6221\n",
      "         1.0       1.00      0.97      0.98       730\n",
      "\n",
      "    accuracy                           1.00      6951\n",
      "   macro avg       1.00      0.98      0.99      6951\n",
      "weighted avg       1.00      1.00      1.00      6951\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.99      0.95      3094\n",
      "         1.0       0.42      0.05      0.10       331\n",
      "\n",
      "    accuracy                           0.90      3425\n",
      "   macro avg       0.66      0.52      0.52      3425\n",
      "weighted avg       0.86      0.90      0.87      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normal_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on undersampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       730\n",
      "         1.0       1.00      1.00      1.00       730\n",
      "\n",
      "    accuracy                           1.00      1460\n",
      "   macro avg       1.00      1.00      1.00      1460\n",
      "weighted avg       1.00      1.00      1.00      1460\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.64      0.76      3094\n",
      "         1.0       0.17      0.69      0.27       331\n",
      "\n",
      "    accuracy                           0.64      3425\n",
      "   macro avg       0.56      0.67      0.52      3425\n",
      "weighted avg       0.88      0.64      0.72      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "undersampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      " For Oversampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      6221\n",
      "         1.0       1.00      0.98      0.99      6221\n",
      "\n",
      "    accuracy                           0.99     12442\n",
      "   macro avg       0.99      0.99      0.99     12442\n",
      "weighted avg       0.99      0.99      0.99     12442\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.99      0.95      3094\n",
      "         1.0       0.39      0.08      0.14       331\n",
      "\n",
      "    accuracy                           0.90      3425\n",
      "   macro avg       0.65      0.54      0.54      3425\n",
      "weighted avg       0.86      0.90      0.87      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on data from dataset\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95      6221\n",
      "         1.0       1.00      0.02      0.03       730\n",
      "\n",
      "    accuracy                           0.90      6951\n",
      "   macro avg       0.95      0.51      0.49      6951\n",
      "weighted avg       0.91      0.90      0.85      6951\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95      3094\n",
      "         1.0       0.00      0.00      0.00       331\n",
      "\n",
      "    accuracy                           0.90      3425\n",
      "   macro avg       0.45      0.50      0.47      3425\n",
      "weighted avg       0.82      0.90      0.86      3425\n",
      "\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "normal_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on undersampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       730\n",
      "         1.0       0.83      0.85      0.84       730\n",
      "\n",
      "    accuracy                           0.83      1460\n",
      "   macro avg       0.84      0.83      0.83      1460\n",
      "weighted avg       0.84      0.83      0.83      1460\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.63      0.76      3094\n",
      "         1.0       0.17      0.72      0.28       331\n",
      "\n",
      "    accuracy                           0.64      3425\n",
      "   macro avg       0.56      0.67      0.52      3425\n",
      "weighted avg       0.88      0.64      0.71      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "undersampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      " For Oversampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.92      0.94      6221\n",
      "         1.0       0.92      0.97      0.95      6221\n",
      "\n",
      "    accuracy                           0.94     12442\n",
      "   macro avg       0.95      0.94      0.94     12442\n",
      "weighted avg       0.95      0.94      0.94     12442\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.86      0.89      3094\n",
      "         1.0       0.21      0.34      0.26       331\n",
      "\n",
      "    accuracy                           0.81      3425\n",
      "   macro avg       0.57      0.60      0.57      3425\n",
      "weighted avg       0.85      0.81      0.83      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on data from dataset\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      6221\n",
      "         1.0       1.00      1.00      1.00       730\n",
      "\n",
      "    accuracy                           1.00      6951\n",
      "   macro avg       1.00      1.00      1.00      6951\n",
      "weighted avg       1.00      1.00      1.00      6951\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90      3094\n",
      "         1.0       0.15      0.18      0.16       331\n",
      "\n",
      "    accuracy                           0.82      3425\n",
      "   macro avg       0.53      0.54      0.53      3425\n",
      "weighted avg       0.84      0.82      0.83      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normal_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on undersampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       730\n",
      "         1.0       1.00      1.00      1.00       730\n",
      "\n",
      "    accuracy                           1.00      1460\n",
      "   macro avg       1.00      1.00      1.00      1460\n",
      "weighted avg       1.00      1.00      1.00      1460\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.54      0.69      3094\n",
      "         1.0       0.13      0.61      0.21       331\n",
      "\n",
      "    accuracy                           0.55      3425\n",
      "   macro avg       0.53      0.58      0.45      3425\n",
      "weighted avg       0.85      0.55      0.64      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "undersampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      " For Oversampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      6221\n",
      "         1.0       1.00      1.00      1.00      6221\n",
      "\n",
      "    accuracy                           1.00     12442\n",
      "   macro avg       1.00      1.00      1.00     12442\n",
      "weighted avg       1.00      1.00      1.00     12442\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.82      0.87      3094\n",
      "         1.0       0.15      0.28      0.19       331\n",
      "\n",
      "    accuracy                           0.77      3425\n",
      "   macro avg       0.53      0.55      0.53      3425\n",
      "weighted avg       0.84      0.77      0.80      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Without sampling\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=x_train[0].shape),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(50,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "model.compile(metrics = 'accuracy',optimizer='adam',loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10,verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nTraining\n               precision    recall  f1-score   support\n\n         0.0       0.94      1.00      0.97      6221\n         1.0       0.95      0.45      0.61       730\n\n    accuracy                           0.94      6951\n   macro avg       0.94      0.72      0.79      6951\nweighted avg       0.94      0.94      0.93      6951\n\nTesting\n               precision    recall  f1-score   support\n\n         0.0       0.91      0.98      0.94      3094\n         1.0       0.33      0.09      0.14       331\n\n    accuracy                           0.89      3425\n   macro avg       0.62      0.53      0.54      3425\nweighted avg       0.85      0.89      0.87      3425\n\n"
     ]
    }
   ],
   "source": [
    "c = classification_report(y_train,model.predict_classes(x_train))\n",
    "print(\"\\n\\nTraining\\n\",c)\n",
    "c = classification_report(y_test,model.predict_classes(x_test))\n",
    "print(\"Testing\\n\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ff1cb958c8>"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "# With undersampling\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=x_u[0].shape),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(50,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(metrics = 'accuracy',optimizer='adam',loss='binary_crossentropy')\n",
    "model.fit(x_u,y_u,validation_data=(x_test,y_test),epochs=10,verbose = 5)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training\n               precision    recall  f1-score   support\n\n         0.0       0.92      0.96      0.94       730\n         1.0       0.96      0.91      0.94       730\n\n    accuracy                           0.94      1460\n   macro avg       0.94      0.94      0.94      1460\nweighted avg       0.94      0.94      0.94      1460\n\nTesting\n               precision    recall  f1-score   support\n\n         0.0       0.94      0.67      0.78      3094\n         1.0       0.17      0.63      0.26       331\n\n    accuracy                           0.66      3425\n   macro avg       0.56      0.65      0.52      3425\nweighted avg       0.87      0.66      0.73      3425\n\n"
     ]
    }
   ],
   "source": [
    "c = classification_report(y_u,model.predict_classes(x_u))\n",
    "print(\"Training\\n\",c)\n",
    "c = classification_report(y_test,model.predict_classes(x_test))\n",
    "print(\"Testing\\n\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ff1cf1d708>"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "# With oversampling\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=x_o[0].shape),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(50,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(metrics = 'accuracy',optimizer='adam',loss='binary_crossentropy')\n",
    "model.fit(x_o,y_o,validation_data=(x_test,y_test),epochs=10,verbose = 5)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training\n               precision    recall  f1-score   support\n\n         0.0       1.00      0.99      1.00      6221\n         1.0       0.99      1.00      1.00      6221\n\n    accuracy                           1.00     12442\n   macro avg       1.00      1.00      1.00     12442\nweighted avg       1.00      1.00      1.00     12442\n\nTesting\n               precision    recall  f1-score   support\n\n         0.0       0.92      0.92      0.92      3094\n         1.0       0.24      0.24      0.24       331\n\n    accuracy                           0.85      3425\n   macro avg       0.58      0.58      0.58      3425\nweighted avg       0.85      0.85      0.85      3425\n\n"
     ]
    }
   ],
   "source": [
    "c = classification_report(y_o,model.predict_classes(x_o))\n",
    "print(\"Training\\n\",c)\n",
    "c = classification_report(y_test,model.predict_classes(x_test))\n",
    "print(\"Testing\\n\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on data from dataset\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.99      0.95      6221\n",
      "         1.0       0.62      0.11      0.18       730\n",
      "\n",
      "    accuracy                           0.90      6951\n",
      "   macro avg       0.76      0.55      0.56      6951\n",
      "weighted avg       0.87      0.90      0.87      6951\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.99      0.95      3094\n",
      "         1.0       0.40      0.08      0.13       331\n",
      "\n",
      "    accuracy                           0.90      3425\n",
      "   macro avg       0.65      0.53      0.54      3425\n",
      "weighted avg       0.86      0.90      0.87      3425\n",
      "\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "normal_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on undersampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.77      0.76       730\n",
      "         1.0       0.76      0.75      0.76       730\n",
      "\n",
      "    accuracy                           0.76      1460\n",
      "   macro avg       0.76      0.76      0.76      1460\n",
      "weighted avg       0.76      0.76      0.76      1460\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.64      0.76      3094\n",
      "         1.0       0.17      0.68      0.27       331\n",
      "\n",
      "    accuracy                           0.64      3425\n",
      "   macro avg       0.56      0.66      0.51      3425\n",
      "weighted avg       0.87      0.64      0.71      3425\n",
      "\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "undersampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      " For Oversampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.74      0.76      6221\n",
      "         1.0       0.75      0.79      0.77      6221\n",
      "\n",
      "    accuracy                           0.76     12442\n",
      "   macro avg       0.77      0.76      0.76     12442\n",
      "weighted avg       0.77      0.76      0.76     12442\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.70      0.81      3094\n",
      "         1.0       0.18      0.62      0.28       331\n",
      "\n",
      "    accuracy                           0.69      3425\n",
      "   macro avg       0.56      0.66      0.54      3425\n",
      "weighted avg       0.87      0.69      0.75      3425\n",
      "\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "oversampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on data from dataset\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      6221\n",
      "         1.0       1.00      1.00      1.00       730\n",
      "\n",
      "    accuracy                           1.00      6951\n",
      "   macro avg       1.00      1.00      1.00      6951\n",
      "weighted avg       1.00      1.00      1.00      6951\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95      3094\n",
      "         1.0       0.00      0.00      0.00       331\n",
      "\n",
      "    accuracy                           0.90      3425\n",
      "   macro avg       0.45      0.50      0.47      3425\n",
      "weighted avg       0.82      0.90      0.86      3425\n",
      "\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Manas Vardhan\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "normal_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification on undersampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       730\n",
      "         1.0       1.00      1.00      1.00       730\n",
      "\n",
      "    accuracy                           1.00      1460\n",
      "   macro avg       1.00      1.00      1.00      1460\n",
      "weighted avg       1.00      1.00      1.00      1460\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.64      0.77      3094\n",
      "         1.0       0.17      0.70      0.28       331\n",
      "\n",
      "    accuracy                           0.65      3425\n",
      "   macro avg       0.56      0.67      0.52      3425\n",
      "weighted avg       0.88      0.65      0.72      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "undersampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      " For Oversampled data\n",
      "\n",
      "\n",
      "\n",
      "Training report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      6221\n",
      "         1.0       1.00      1.00      1.00      6221\n",
      "\n",
      "    accuracy                           1.00     12442\n",
      "   macro avg       1.00      1.00      1.00     12442\n",
      "weighted avg       1.00      1.00      1.00     12442\n",
      "\n",
      "\n",
      "\n",
      "Testing report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.98      0.94      3094\n",
      "         1.0       0.31      0.08      0.12       331\n",
      "\n",
      "    accuracy                           0.89      3425\n",
      "   macro avg       0.61      0.53      0.53      3425\n",
      "weighted avg       0.85      0.89      0.86      3425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oversampled_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}